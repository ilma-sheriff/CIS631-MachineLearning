{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "#Import nltk to check english lexicon\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import (\n",
    "    wordnet,\n",
    "    stopwords\n",
    ")\n",
    "\n",
    "#import libraries for tokenization and ML\n",
    "import json;\n",
    "import keras;\n",
    "import keras.preprocessing.text as kpt;\n",
    "#from keras.preprocessing.text import Tokenizer;\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Import all libraries for creating a deep neural network\n",
    "#Sequential is the standard type of neural network with stackable layers\n",
    "from keras.models import (\n",
    "    Sequential,\n",
    "    model_from_json\n",
    ")\n",
    "#Dense: Standard layers with every node connected, dropout: avoids overfitting\n",
    "from keras.layers import Dense, Dropout, Activation;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv(\"/Users/ilmasheriff/tweetdata\",encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010508, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Data</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cnsnews benshapiro Can COVID19Vaccine end the ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>IntrepidWarrior realDonaldTrump Can COVID19Vac...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>edyong209 HelenBranswell Can COVID19Vaccine en...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AdamTexDavis Can COVID19Vaccine end the pandemic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HegKong Can COVID19Vaccine end the pandemic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Data  \\\n",
       "0           0  cnsnews benshapiro Can COVID19Vaccine end the ...   \n",
       "1           1  IntrepidWarrior realDonaldTrump Can COVID19Vac...   \n",
       "2           2  edyong209 HelenBranswell Can COVID19Vaccine en...   \n",
       "3           3   AdamTexDavis Can COVID19Vaccine end the pandemic   \n",
       "4           4        HegKong Can COVID19Vaccine end the pandemic   \n",
       "\n",
       "   Subjectivity  Polarity    Score  \n",
       "0           0.0       0.0  Neutral  \n",
       "1           0.0       0.0  Neutral  \n",
       "2           0.0       0.0  Neutral  \n",
       "3           0.0       0.0  Neutral  \n",
       "4           0.0       0.0  Neutral  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Negative'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.Score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Data</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cnsnews benshapiro Can COVID19Vaccine end the ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>IntrepidWarrior realDonaldTrump Can COVID19Vac...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>edyong209 HelenBranswell Can COVID19Vaccine en...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AdamTexDavis Can COVID19Vaccine end the pandemic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HegKong Can COVID19Vaccine end the pandemic</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899995</th>\n",
       "      <td>899995</td>\n",
       "      <td>BarackObama The whole is literally sat down li...</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899996</th>\n",
       "      <td>899996</td>\n",
       "      <td>I dont know about you but I want this pandemic...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899997</th>\n",
       "      <td>899997</td>\n",
       "      <td>RT CedarsSinai Experts are doing everything th...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899998</th>\n",
       "      <td>899998</td>\n",
       "      <td>US disease expert Fauci says vaccine verdict d...</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899999</th>\n",
       "      <td>899999</td>\n",
       "      <td>RT vankapro If Proved To Be SafeCovid19Vaccine...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               Data  \\\n",
       "0                0  cnsnews benshapiro Can COVID19Vaccine end the ...   \n",
       "1                1  IntrepidWarrior realDonaldTrump Can COVID19Vac...   \n",
       "2                2  edyong209 HelenBranswell Can COVID19Vaccine en...   \n",
       "3                3   AdamTexDavis Can COVID19Vaccine end the pandemic   \n",
       "4                4        HegKong Can COVID19Vaccine end the pandemic   \n",
       "...            ...                                                ...   \n",
       "899995      899995  BarackObama The whole is literally sat down li...   \n",
       "899996      899996  I dont know about you but I want this pandemic...   \n",
       "899997      899997  RT CedarsSinai Experts are doing everything th...   \n",
       "899998      899998  US disease expert Fauci says vaccine verdict d...   \n",
       "899999      899999  RT vankapro If Proved To Be SafeCovid19Vaccine...   \n",
       "\n",
       "        Subjectivity  Polarity     Score  \n",
       "0           0.000000  0.000000   Neutral  \n",
       "1           0.000000  0.000000   Neutral  \n",
       "2           0.000000  0.000000   Neutral  \n",
       "3           0.000000  0.000000   Neutral  \n",
       "4           0.000000  0.000000   Neutral  \n",
       "...              ...       ...       ...  \n",
       "899995      0.344444  0.022222  Positive  \n",
       "899996      0.000000  0.000000   Neutral  \n",
       "899997      0.000000  0.000000   Neutral  \n",
       "899998      0.337500 -0.012500  Negative  \n",
       "899999      0.000000  0.000000   Neutral  \n",
       "\n",
       "[900000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextAnalysis(a):\n",
    "    if a > 0:\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['Score'] = table['Polarity'].apply(getTextAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x144bb9b20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table.Score.value_counts().plot(kind='pie', autopct='%2.0f%%', colors=[\"red\",\"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess text in tweets by removing links, @UserNames, blank spaces, etc.\n",
    "def preprocessing_text(table):\n",
    "    #put everythin in lowercase\n",
    "    table['Data'] = table['Data'].str.lower()\n",
    "    #Replace rt indicating that was a retweet\n",
    "    table['Data'] = table['Data'].str.replace('rt', '')\n",
    "    #Replace occurences of mentioning @UserNames\n",
    "    table['Data'] = table['Data'].replace(r'@\\w+', '', regex=True)\n",
    "    #Replace links contained in the tweet\n",
    "    table['Data'] = table['Data'].replace(r'http\\S+', '', regex=True)\n",
    "    table['Data'] = table['Data'].replace(r'www.[^ ]+', '', regex=True)\n",
    "    #remove numbers\n",
    "    table['Data'] = table['Data'].replace(r'[0-9]+', '', regex=True)\n",
    "    #replace special characters and puntuation marks\n",
    "    table['Data'] = table['Data'].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace elongated words by identifying those repeated characters and then remove them and compare the new word with the english lexicon\n",
    "def in_dict(word):\n",
    "    if wordnet.synsets(word):\n",
    "        #if the word is in the dictionary, we'll return True\n",
    "        return True\n",
    "\n",
    "def replace_elongated_word(word):\n",
    "    regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
    "    repl = r'\\1\\2\\3'    \n",
    "    if in_dict(word):\n",
    "        return word\n",
    "    new_word = re.sub(regex, repl, word)\n",
    "    if new_word != word:\n",
    "        return replace_elongated_word(new_word)\n",
    "    else:\n",
    "        return new_word\n",
    "\n",
    "def detect_elongated_words(row):\n",
    "    regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
    "    words = [''.join(i) for i in re.findall(regexrep, row)]\n",
    "    for word in words:\n",
    "        if not in_dict(word):\n",
    "            row = re.sub(word, replace_elongated_word(word), row)\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(table):\n",
    "    #We need to remove the stop words\n",
    "    stop_words_list = stopwords.words('english')\n",
    "    table['Data'] = table['Data'].str.lower()\n",
    "    table['Data'] = table['Data'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_antonyms(word):\n",
    "    #We get all the lemma for the word\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for lemma in syn.lemmas(): \n",
    "            #if the lemma is an antonyms of the word\n",
    "            if lemma.antonyms(): \n",
    "                #we return the antonym\n",
    "                return lemma.antonyms()[0].name()\n",
    "    return word\n",
    "            \n",
    "def handling_negation(row):\n",
    "    #Tokenize the row\n",
    "    words = word_tokenize(row)\n",
    "    speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
    "    #We obtain the type of words that we have in the text, we use the pos_tag function\n",
    "    tags = nltk.pos_tag(words)\n",
    "    #Now we ask if we found a negation in the words\n",
    "    tags_2 = ''\n",
    "    if \"n't\" in words and \"not\" in words:\n",
    "        tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "        words_2 = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "        words = words[:(min(words.index(\"n't\"), words.index(\"not\")))+1]\n",
    "    elif \"n't\" in words:\n",
    "        tags_2 = tags[words.index(\"n't\"):]\n",
    "        words_2 = words[words.index(\"n't\"):] \n",
    "        words = words[:words.index(\"n't\")+1]\n",
    "    elif \"not\" in words:\n",
    "        tags_2 = tags[words.index(\"not\"):]\n",
    "        words_2 = words[words.index(\"not\"):]\n",
    "        words = words[:words.index(\"not\")+1] \n",
    "        \n",
    "    for index, word_tag in enumerate(tags_2):\n",
    "        if word_tag[1] in speach_tags:\n",
    "            words = words+[replace_antonyms(word_tag[0])]+words_2[index+2:]\n",
    "            break\n",
    "            \n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_table(table):\n",
    "    #This function will process all the required cleaning for the text in our tweets\n",
    "    table = preprocessing_text(table)\n",
    "    table['Data'] = table['Data'].apply(lambda x: detect_elongated_words(x))\n",
    "    table['Data'] = table['Data'].apply(lambda x: handling_negation(x))\n",
    "    table = stop_words(table)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization for Data Visualization\n",
    "def vectorization(table):\n",
    "    #CountVectorizer will convert a collection of text documents to a matrix of token counts\n",
    "    #Produces a sparse representation of the counts \n",
    "    #Initialize\n",
    "    vector = CountVectorizer()\n",
    "    #We fit and transform the vector created\n",
    "    frequency_matrix = vector.fit_transform(table.Data)\n",
    "    #Sum all the frequencies for each word\n",
    "    sum_frequencies = np.sum(frequency_matrix, axis=0)\n",
    "    #Now we use squeeze to remove single-dimensional entries from the shape of an array that we got from applying np.asarray to\n",
    "    #the sum of frequencies.\n",
    "    frequency = np.squeeze(np.asarray(sum_frequencies))\n",
    "    #Now we get into a dataframe all the frequencies and the words that they correspond to\n",
    "    frequency_df = pd.DataFrame([frequency], columns=vector.get_feature_names()).transpose()\n",
    "    return frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(tweets):\n",
    "    \n",
    "    #We get the directory that we are working on\n",
    "    file = os.getcwd()\n",
    "\n",
    "    word_cloud = WordCloud(width=2000, height=1000, max_font_size=200, background_color=\"black\", max_words=2000,  contour_width=1, \n",
    "                           contour_color=\"steelblue\", colormap=\"nipy_spectral\", stopwords=[\"covid19\"])\n",
    "    word_cloud.generate(tweets)\n",
    "    \n",
    "    #wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(tweets_list)\n",
    "    \n",
    "    #Now we plot both figures, the wordcloud and the mask\n",
    "    #plt.figure(figsize=(15,15))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(word_cloud, interpolation=\"hermite\")\n",
    "    plt.axis(\"off\")\n",
    "    #plt.imshow(avengers_mask, cmap=plt.cm.gray, interpolation=\"bilinear\")\n",
    "    #plt.axis(\"off\")    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(word_frequency, sent):\n",
    "    labels = word_frequency[0][1:51].index\n",
    "    title = \"Word Frequency for %s\" %sent\n",
    "    #Plot the figures\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(np.arange(50), word_frequency[0][1:51], width = 0.8, color = sns.color_palette(\"bwr\"), alpha=0.5, \n",
    "            edgecolor = \"black\", capsize=8, linewidth=1);\n",
    "    plt.xticks(np.arange(50), labels, rotation=90, size=14);\n",
    "    plt.xlabel(\"50 more frequent words\", size=14);\n",
    "    plt.ylabel(\"Frequency\", size=14);\n",
    "    #plt.title('Word Frequency for %s', size=18) %sent;\n",
    "    plt.title(title, size=18)\n",
    "    plt.grid(False);\n",
    "    plt.gca().spines[\"top\"].set_visible(False);\n",
    "    plt.gca().spines[\"right\"].set_visible(False);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_graph(table):\n",
    "    table = table[1:]\n",
    "    #We set the style of seaborn\n",
    "    sns.set_style(\"whitegrid\")   \n",
    "    #Initialize the figure\n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    #we obtain the points from matplotlib scatter\n",
    "    points = plt.scatter(table[\"1\"], table[\"0\"], c=table[\"1\"], s=75, cmap=\"bwr\")\n",
    "    #graph the colorbar\n",
    "    plt.colorbar(points)\n",
    "    #we graph the regplot from seaborn\n",
    "    sns.regplot(x=\"1\", y=\"0\",fit_reg=False, scatter=False, color=\".1\", data=table)\n",
    "    plt.xlabel(\"Frequency for Positive Tweets\", size=14)\n",
    "    plt.ylabel(\"Frequency for Negative Tweets\", size=14)\n",
    "    plt.title(\"Word frequency in Positive vs. Negative Tweets\", size=14)\n",
    "    plt.grid(False)\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into training and test dataset\n",
    "def splitting(table):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(table.Data, table.Score, test_size=0.2, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization for analysis\n",
    "def tokenization_tweets(dataset, features):\n",
    "    tokenization = TfidfVectorizer(max_features=features)\n",
    "    tokenization.fit(dataset)\n",
    "    dataset_transformed = tokenization.transform(dataset).toarray()\n",
    "    return dataset_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, model_nn):\n",
    "    prediction = model_nn.predict(X_test)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_table = cleaning_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_table[tweet_table['Score'] == \"0\"][\"Data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Get the frequency\n",
    "    word_frequency = vectorization(tweet_table).sort_values(0, ascending = False)\n",
    "    word_frequency_pos = vectorization(tweet_table[tweet_table['Score'] == '1']).sort_values(0, ascending = False)\n",
    "    word_frequency_neg = vectorization(tweet_table[tweet_table['Score'] == '0']).sort_values(0, ascending = False)\n",
    "\n",
    "    #Graph with frequency words all, positive and negative tweets and get the frequency\n",
    "    graph(word_frequency, 'all')\n",
    "    graph(word_frequency_pos, '1')\n",
    "    graph(word_frequency_neg, '0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Concatenate word frequency for positive and negative\n",
    "    table_regression = pd.concat([word_frequency_pos, word_frequency_neg], axis=1, sort=False)\n",
    "    table_regression.columns = [\"1\", \"0\"]\n",
    "    regression_graph(table_regression)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tweet_table = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tweet_table['Score'] = tweet_table['Score'].apply(lambda x: 2 if x == '2' else (0 if x == '1' else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_train, X_test, y_train, y_test = splitting(tweet_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Neural Network\n",
    "#Create the model\n",
    "def train(X_train_mod, y_train, features, shuffle, drop, layer1, layer2, layer3,layer4,layer5, layer6,layer7,epoch, lr, epsilon, validation):\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(layer1, input_shape=(features,), activation='relu'))\n",
    "    model_nn.add(Dropout(drop))\n",
    "    model_nn.add(Dense(layer2, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(drop))\n",
    "    model_nn.add(Dense(layer3, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(drop))\n",
    "    model_nn.add(Dense(layer4, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(drop))\n",
    "    model_nn.add(Dense(layer5, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(drop))\n",
    "    model_nn.add(Dense(layer6, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(drop))\n",
    "    model_nn.add(Dense(layer7, activation='sigmoid'))\n",
    "    model_nn.add(Dropout(drop))\n",
    "    \n",
    "    model_nn.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=epsilon, decay=0.0, amsgrad=False)\n",
    "    model_nn.compile(loss='binary_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "#     , 'precision','recall','auc'\n",
    "    model_nn.fit(np.array(X_train_mod), y_train,\n",
    "                 batch_size=32,\n",
    "                 epochs=epoch,\n",
    "                 verbose=1,\n",
    "                 validation_split=validation,\n",
    "                 shuffle=shuffle)\n",
    "    return model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(X_train, y_train):   \n",
    "    features = 3000\n",
    "    shuffle = True\n",
    "    drop = 0.5\n",
    "    layer1 = 600\n",
    "    layer2 = 512\n",
    "    layer3 = 256\n",
    "    layer4 = 200\n",
    "    layer5 = 112\n",
    "    layer6 = 56\n",
    "    layer7 = 16\n",
    "    epoch = 10\n",
    "    lr = 0.002\n",
    "    epsilon = 0.00008\n",
    "    validation = 0.1\n",
    "    X_train_mod = tokenization_tweets(X_train, features)\n",
    "    model = train(X_train_mod, y_train, features, shuffle, drop, layer1, layer2,layer3, layer4,layer5, layer6, layer7, epoch, lr, epsilon, validation)\n",
    "    return model;\n",
    "model1(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(X_train, y_train):   \n",
    "    features = 3000\n",
    "    shuffle = True\n",
    "    drop = 0.5\n",
    "    layer1 = 600\n",
    "    layer2 = 512\n",
    "    layer3 = 256\n",
    "    layer4 = 200\n",
    "    epoch = 5\n",
    "    lr = 0.002\n",
    "    epsilon = 0.00000001\n",
    "    validation = 0.1\n",
    "    X_train_mod = tokenization_tweets(X_train, features)\n",
    "    model = train(X_train_mod, y_train, features, shuffle, drop, layer1, layer2,layer3, layer4, epoch, lr, epsilon, validation)\n",
    "    return model;\n",
    "model2(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3(X_train, y_train):   \n",
    "    features = 3000\n",
    "    shuffle = True\n",
    "    drop = 0.5\n",
    "    layer1 = 600\n",
    "    layer2 = 512\n",
    "    layer3 = 256\n",
    "    layer4 = 200\n",
    "    epoch = 5\n",
    "    lr = 0.009\n",
    "    epsilon = 0.001\n",
    "    validation = 0.2\n",
    "    X_train_mod = tokenization_tweets(X_train, features)\n",
    "    model = train(X_train_mod, y_train, features, shuffle, drop, layer1, layer2,layer3, layer4, epoch, lr, epsilon, validation)\n",
    "    return model;\n",
    "model3(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4(X_train, y_train):   \n",
    "    features = 3000\n",
    "    shuffle = True\n",
    "    drop = 0.5\n",
    "    layer1 = 540\n",
    "    layer2 = 532\n",
    "    layer3 = 256\n",
    "    layer4 = 230\n",
    "    layer5 = 112\n",
    "    layer6 = 56\n",
    "    layer7 = 10\n",
    "    epoch = 5\n",
    "    lr = 0.002\n",
    "    epsilon = 0.00000001\n",
    "    validation = 0.1\n",
    "    X_train_mod = tokenization_tweets(X_train, features)\n",
    "    model = train(X_train_mod, y_train, features, shuffle, drop, layer1, layer2,layer3, layer4, epoch, lr, epsilon, validation)\n",
    "    return model;\n",
    "model4(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
